{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipulgote1999/machine-learning-udacity/blob/master/decision_tree_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz-NkNn7Q_K_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a0af7520-cc39-458b-94ed-062fee94bc4b"
      },
      "source": [
        "!pip install scikit-learn==0.18\n",
        "#!pip install scipy==0.18.1\n",
        "\n",
        "#!pip install nltk==3.2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/51/4b2c24523ca21e9f3efadff99096067f1fdb20dd0b2a4582d68e63612a59/scikit_learn-0.18-cp27-cp27mu-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 1.4MB/s \n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.20.3\n",
            "    Uninstalling scikit-learn-0.20.3:\n",
            "      Successfully uninstalled scikit-learn-0.20.3\n",
            "Successfully installed scikit-learn-0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "420V3-SsS62b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "3b3c9adc-1241-4543-f44a-eb41400cd4dc"
      },
      "source": [
        "!pip install numpy==1.11.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.11.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/d5/3433e015f3e4a1f309dbb110e8557947f68887fe9b8438d50a4b7790a954/numpy-1.11.2-cp27-cp27mu-manylinux1_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 1.3MB/s \n",
            "\u001b[31mERROR: cvxpy 1.0.15 has requirement numpy>=1.14, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 0.14.0 has requirement numpy>=1.14, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement numpy<2.0,>=1.14.5, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: autograd 1.2 has requirement numpy>=1.12, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-hub 0.5.0 has requirement numpy>=1.12.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 0.24.2 has requirement numpy>=1.12.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.11.3 has requirement numpy>=1.12, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pymc3 3.6 has requirement numpy>=1.13.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 1.14.0 has requirement numpy>=1.12.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.7.0 has requirement numpy>=1.13.3, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.3.9 has requirement numpy>=1.13, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: featuretools 0.4.1 has requirement numpy>=1.13.3, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.1.6 has requirement numpy>=1.15.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement numpy>=1.13.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gensim 3.6.0 has requirement numpy>=1.11.3, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed numpy-1.11.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szo4CZMnREm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "55a982a8-2816-4f28-9d47-0bdf89cb3de1"
      },
      "source": [
        "!git clone https://github.com/udacity/ud120-projects.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ud120-projects'...\n",
            "remote: Enumerating objects: 5046, done.\u001b[K\n",
            "remote: Total 5046 (delta 0), reused 0 (delta 0), pack-reused 5046\n",
            "Receiving objects: 100% (5046/5046), 19.64 MiB | 8.58 MiB/s, done.\n",
            "Resolving deltas: 100% (4372/4372), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9f35ePASAaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade bitcount"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_NRzrJDQg3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a1e3f657-effd-412a-ba32-8a984f496d67"
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "import pickle\n",
        "import cPickle\n",
        "import numpy\n",
        "\n",
        "from sklearn import cross_validation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(words_file = \"/content/ud120-projects/tools/word_data.pkl\", authors_file=\"/content/ud120-projects/tools/email_authors.pkl\"):\n",
        "    \"\"\" \n",
        "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
        "        and the corresponding authors (by default email_authors.pkl) and performs\n",
        "        a number of preprocessing steps:\n",
        "            -- splits into training/testing sets (10% testing)\n",
        "            -- vectorizes into tfidf matrix\n",
        "            -- selects/keeps most helpful features\n",
        "\n",
        "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
        "\n",
        "        4 objects are returned:\n",
        "            -- training/testing features\n",
        "            -- training/testing labels\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### the words (features) and authors (labels), already largely preprocessed\n",
        "    ### this preprocessing will be repeated in the text learning mini-project\n",
        "    authors_file_handler = open(authors_file, \"r\")\n",
        "    authors = pickle.load(authors_file_handler)\n",
        "    authors_file_handler.close()\n",
        "\n",
        "    words_file_handler = open(words_file, \"r\")\n",
        "    word_data = cPickle.load(words_file_handler)\n",
        "    words_file_handler.close()\n",
        "\n",
        "    ### test_size is the percentage of events assigned to the test set\n",
        "    ### (remainder go into training)\n",
        "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    ### text vectorization--go from strings to lists of numbers\n",
        "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
        "                                 stop_words='english')\n",
        "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
        "    features_test_transformed  = vectorizer.transform(features_test)\n",
        "\n",
        "\n",
        "\n",
        "    ### feature selection, because text is super high dimensional and \n",
        "    ### can be really computationally chewy as a result\n",
        "    selector = SelectPercentile(f_classif, percentile=1)\n",
        "    selector.fit(features_train_transformed, labels_train)\n",
        "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
        "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
        "\n",
        "    ### info on the data\n",
        "    print \"no. of Chris training emails:\", sum(labels_train)\n",
        "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
        "    \n",
        "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imJfF0AppHgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "fa1182b6-f61e-48f5-e25a-6ec3a689171b"
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "\"\"\" \n",
        "    This is the code to accompany the Lesson 3 (decision tree) mini-project.\n",
        "    Use a Decision Tree to identify emails from the Enron corpus by author:    \n",
        "    Sara has label 0\n",
        "    Chris has label 1\n",
        "\"\"\"\n",
        "    \n",
        "import sys\n",
        "from time import time\n",
        "sys.path.append(\"../tools/\")\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "### features_train and features_test are the features for the training\n",
        "### and testing datasets, respectively\n",
        "### labels_train and labels_test are the corresponding item labels\n",
        "features_train, features_test, labels_train, labels_test = preprocess()\n",
        "\n",
        "clf=tree.DecisionTreeClassifier(min_samples_split=40)\n",
        "clf.fit(features_train,labels_train)\n",
        "\n",
        "pred=clf.predict(features_test)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
            "/usr/local/lib/python2.7/dist-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(mask.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "no. of Chris training emails: 7936\n",
            "no. of Sara training emails: 7884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU56QlT2UmYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "26fdbe51-1d7e-41cf-955f-5752fc49d5ee"
      },
      "source": [
        "print(len(pred))\n",
        "accuracy=accuracy_score(labels_test,pred)\n",
        "print(accuracy)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1758\n",
            "0.9664391353811149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd0oG2V8qHk5",
        "colab_type": "text"
      },
      "source": [
        "# **for printing the no of features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2Ztp48KYrl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5492188a-57dd-485d-a2ed-435e61b7587f"
      },
      "source": [
        "len(features_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "379"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}