{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text stemming udacity .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipulgote1999/machine-learning-udacity/blob/master/text_stemming_udacity_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrMjsp-IuyHi",
        "colab_type": "code",
        "outputId": "781e0c04-34e5-4838-86cf-be41f7ad1dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!git clone 'https://github.com/udacity/ud120-projects.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ud120-projects'...\n",
            "remote: Enumerating objects: 5046, done.\u001b[K\n",
            "remote: Total 5046 (delta 0), reused 0 (delta 0), pack-reused 5046\u001b[K\n",
            "Receiving objects: 100% (5046/5046), 19.64 MiB | 25.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4372/4372), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LchUQPuC84d",
        "colab_type": "code",
        "outputId": "45861e58-3622-4ba6-b4c2-3a26d14f8b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "\n",
        "def parseOutText(f):\n",
        "    \"\"\" given an opened email file f, parse out all text below the\n",
        "        metadata block at the top\n",
        "        (in Part 2, you will also add stemming capabilities)\n",
        "        and return a string that contains all the words\n",
        "        in the email (space-separated) \n",
        "        \n",
        "        example use case:\n",
        "        f = open(\"email_file_name.txt\", \"r\")\n",
        "        text = parseOutText(f)\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "    f.seek(0)  ### go back to beginning of file (annoying)\n",
        "    all_text = f.read()\n",
        "\n",
        "    ### split off metadata\n",
        "    content = all_text.split(\"X-FileName:\")\n",
        "    words = \"\"\n",
        "    if len(content) > 1:\n",
        "        ### remove punctuation\n",
        "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
        "\n",
        "        ### project part 2: comment out the line below\n",
        "        words = text_string\n",
        "\n",
        "        ### split the text string into individual words, stem each word,\n",
        "        ### and append the stemmed word to words (make sure there's a single\n",
        "        ### space between each stemmed word)\n",
        " \n",
        "    return words\n",
        "\n",
        "    \n",
        "\n",
        "def main():\n",
        "    ff = open(\"/content/ud120-projects/text_learning/test_email.txt\", \"r\")\n",
        "    text = parseOutText(ff)\n",
        "    print text\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0O1HQMswhSG",
        "colab_type": "code",
        "outputId": "5d6e9f2a-0852-4786-8e2f-13f4b91eef55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "\n",
        "def parseOutText(f):\n",
        "    \"\"\" given an opened email file f, parse out all text below the\n",
        "        metadata block at the top\n",
        "        (in Part 2, you will also add stemming capabilities)\n",
        "        and return a string that contains all the words\n",
        "        in the email (space-separated) \n",
        "        \n",
        "        example use case:\n",
        "        f = open(\"email_file_name.txt\", \"r\")\n",
        "        text = parseOutText(f)\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    f.seek(0)  ### go back to beginning of file (annoying)\n",
        "    all_text = f.read()\n",
        "\n",
        "    ### split off metadata\n",
        "    content = all_text.split(\"X-FileName:\")\n",
        "    words = \"\"\n",
        "    if len(content) > 1:\n",
        "        ### remove punctuation\n",
        "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
        "        words=[]\n",
        "        print(\"text_string(input text ): {}\".format(text_string))\n",
        "        ### project part 2: comment out the line below\n",
        "        #words = text_string\n",
        "        s=text_string.split()\n",
        "        print('splitted text:{}'.format(s))\n",
        "        #print(s)\n",
        "        for i in s:\n",
        "          stemmer=SnowballStemmer(\"english\")\n",
        "          word=stemmer.stem(i)\n",
        "          words.append(word)\n",
        "\n",
        "        ### split the text string into individual words, stem each word,\n",
        "        ### and append the stemmed word to words (make sure there's a single\n",
        "        ### space between each stemmed word)\n",
        "\n",
        "\n",
        "    return words\n",
        "\n",
        "def main():\n",
        "    ff = open(\"/content/ud120-projects/text_learning/test_email.txt\", \"r\")\n",
        "    text = parseOutText(ff)\n",
        "    print ('\\n stemmed text after splitting into individual words: \\n {}'.format(text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_string(input text ): \n",
            "\n",
            "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
            "\n",
            "splitted text:['Hi', 'Everyone', 'If', 'you', 'can', 'read', 'this', 'message', 'youre', 'properly', 'using', 'parseOutText', 'Please', 'proceed', 'to', 'the', 'next', 'part', 'of', 'the', 'project']\n",
            "\n",
            " stemmed text after splitting into individual words: \n",
            " ['hi', u'everyon', 'if', u'you', u'can', u'read', u'this', u'messag', u'your', u'proper', u'use', u'parseouttext', u'pleas', u'proceed', 'to', u'the', u'next', u'part', 'of', u'the', u'project']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}